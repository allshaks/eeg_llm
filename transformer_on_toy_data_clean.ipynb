{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/u_kamm_software/EEG data analysis/eeg_llm/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerForPrediction\n",
    "import numpy as np \n",
    "from torch.optim import Adam \n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define global variables \n",
    "NUM_OBSV = 6000                     # number of generated observations  \n",
    "SEQ_LEN = 100                       # length of the sequence \n",
    "NOISE = 1                           # level of noise \n",
    "MEAN1 = 1                           # mean of the first pulse \n",
    "STD1 = 0.3                          # standard deviation of the first pulse \n",
    "MEAN2 = 2                           # mean of the second pulse \n",
    "STD2 = 0.3                          # standard deviation of the second pulse \n",
    "TEST_SIZE = 0.2                     # size of the test set \n",
    "BATCH_SIZE = 16                     # batch size \n",
    "RANDOM_STATE = 42                   # random initialization for reproducability \n",
    "PRED_LENGTH = 70                    # length of the prediction \n",
    "CONTEXT_LENGTH = 29                 # length of the context window \n",
    "NUM_TIME_FEAT = 1                   # number of time features \n",
    "ENC_LAYERS = 2                      # number of encoding layers \n",
    "DEC_LAYERS = 2                      # number of decoding layers \n",
    "INPUT_FEAT = 2                      # number of input features \n",
    "LAG_SEQ = [1]                       # lag sequence \n",
    "D_MODEL = 64                        # dimension of the model \n",
    "N_HEADS = 8                         # number of attention heads\n",
    "DISTR = 'normal'                    # output distribution  \n",
    "NUM_SAMPLES = 10                    # number of parallel generated samples \n",
    "NUM_EPOCHS = 20                     # number of epochs \n",
    "LEARNING_RATE = 1e-3                # learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(t, mean, std):\n",
    "    \"\"\"\n",
    "    Computes the Gaussian (normal distribution) value for each point in time.\n",
    "\n",
    "    Parameters:\n",
    "    t (array-like): The input time points at which to evaluate the Gaussian function.\n",
    "    mean (float): The mean (center) of the Gaussian distribution.\n",
    "    std (float): The standard deviation (spread or width) of the Gaussian distribution.\n",
    "\n",
    "    Returns:\n",
    "    ndarray: A 1D array of shape (t,)\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1/np.sqrt(2*np.pi*std**2)*np.exp(-(t-mean)**2/(2*std**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pulse_data(num_samples=NUM_OBSV, seq_len=SEQ_LEN, noise=NOISE):\n",
    "    \"\"\"\n",
    "    Generates synthetic pulse data with time-shifted Gaussian pulses and random noise.\n",
    "\n",
    "    Parameters:\n",
    "    num_samples (int): The number of pulse samples to generate. Each sample contains two pulses. \n",
    "    seq_length (int): The number of time points in each pulse sequence.\n",
    "    noise (float): The standard deviation of the noise added to the pulse amplitudes. \n",
    "\n",
    "    Returns:\n",
    "    ndarray: A 3D array of shape (num_samples, seq_length, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        phase_shift = np.random.randn(1)\n",
    "        w = 1 # + np.random.randn(1)*0.01\n",
    "        t = np.linspace(0, 5, seq_len) + phase_shift\n",
    "\n",
    "        pulse1 = gaussian(t, MEAN1, STD1) + noise * np.random.randn(seq_len)\n",
    "        pulse2 = gaussian(t, MEAN2, STD2) + noise * np.random.randn(seq_len)\n",
    "\n",
    "        sample = np.stack([pulse1, pulse2], axis=1)\n",
    "        data.append(sample)\n",
    "    data = np.array(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, test_size=TEST_SIZE, random_state=RANDOM_STATE):\n",
    "    \"\"\"\n",
    "    Splits the input data into training and testing set.\n",
    "\n",
    "    Parameters:\n",
    "    data (array-like): The input data to be split.\n",
    "    test_size (float): The proportion of the data to include in the test split.\n",
    "    random_state (int, optional): Controls the shuffling applied to the data before the split. \n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - X_train (array-like): A 3D array of shape (num_samples - test_size * num_samples, seq_length, 2).\n",
    "        - y (array-like): A 3D array of shape (test_size * num_samples, seq_length, 2).\n",
    "    \"\"\"\n",
    "\n",
    "    X_train, y = train_test_split(data, test_size=test_size, random_state=random_state)\n",
    "    return X_train, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(X_train, X_test, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    \"\"\"\n",
    "    Converts training and testing data into PyTorch DataLoader objects.\n",
    "\n",
    "    Parameters:\n",
    "    X_train (array-like): The training data to be converted into a DataLoader.\n",
    "    X_test (array-like): The test data to be converted into a DataLoader.\n",
    "    batch_size (int): The number of samples per batch to load.\n",
    "    shuffle (bool): Whether to shuffle the training data at every epoch. Default is True.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - train_loader (DataLoader): DataLoader for the training data.\n",
    "        - test_loader (DataLoader): DataLoader for the test data (shuffling disabled).\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    train_dataset = TensorDataset(X_train)\n",
    "    test_dataset = TensorDataset(X_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_past_future(batch, num_future_points=PRED_LENGTH):\n",
    "    \"\"\"\n",
    "    Splits a batch of time series data into past and future segments.\n",
    "\n",
    "    Parameters:\n",
    "    batch (ndarray): A batch of time series data of shape (batch_size, sequence_length, num_features).\n",
    "    num_future_points (int): The number of future time points to separate from the past.\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - past_values (ndarray): The past segment of the data, with shape (batch_size, sequence_length - num_future_points, num_features).\n",
    "        - future_values (ndarray): The future segment of the data, with shape (batch_size, num_future_points, num_features).\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    past_values = batch[:, :-num_future_points, :]\n",
    "    future_values = batch[:, -num_future_points:, :]\n",
    "    return past_values, future_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, num_future_points=PRED_LENGTH, num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE):\n",
    "    \"\"\"\n",
    "    Trains the model using the provided training data loader.\n",
    "\n",
    "    Parameters:\n",
    "    model (torch.nn.Module): The model to be trained.\n",
    "    train_loader (DataLoader): The DataLoader providing batches of training data.\n",
    "    num_future_points (int): The number of future time steps the model will predict. Default is 70.\n",
    "    num_epochs (int): The number of training epochs. Default is 20.\n",
    "    learning_rate (float): The learning rate for the optimizer. Default is 1e-3.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of parameters (scale and location) for each epoch.\n",
    "    \"\"\"\n",
    "    # Initialize optimizer\n",
    "    optim = Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Store parameters across epochs\n",
    "    params_lst = []\n",
    "\n",
    "    # Iterate over all epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        total_loss = 0.0  # Initialize total loss for the epoch\n",
    "        epoch_params = []  # Store parameters for the current epoch\n",
    "\n",
    "        # Iterate over each batch in the training data\n",
    "        for batch in train_loader:\n",
    "            past_values, future_values = split_past_future(batch[0], num_future_points=num_future_points)\n",
    "\n",
    "            # Prepare batch data for model input\n",
    "            batch = {\n",
    "                \"past_values\": past_values,  # (batch_size, input_length, input_size)\n",
    "                \"future_values\": future_values,  # (batch_size, prediction_length, input_size)\n",
    "                \"past_time_features\": torch.arange(past_values.size(1)).unsqueeze(0).unsqueeze(2).float().repeat(past_values.size(0), 1, 1),  # (batch_size, seq_length, 1)\n",
    "                \"past_observed_mask\": torch.ones_like(past_values),  # (batch_size, seq_length, input_size)\n",
    "                \"future_observed_mask\": torch.ones_like(future_values),  # (batch_size, prediction_length, input_size)\n",
    "                \"future_time_features\": torch.arange(past_values.size(1), past_values.size(1) + num_future_points).unsqueeze(0).unsqueeze(2).float().repeat(future_values.size(0), 1, 1),  # (batch_size, prediction_length, 1)\n",
    "                \"return_dict\": True\n",
    "            }\n",
    "\n",
    "            # Forward pass through the model\n",
    "            outputs = model(\n",
    "                past_values=batch[\"past_values\"],\n",
    "                past_time_features=batch[\"past_time_features\"],\n",
    "                past_observed_mask=batch[\"past_observed_mask\"],\n",
    "                future_observed_mask=batch[\"future_observed_mask\"],\n",
    "                future_values=batch[\"future_values\"],\n",
    "                future_time_features=batch[\"future_time_features\"],\n",
    "                return_dict=batch[\"return_dict\"]\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            scale = outputs.scale\n",
    "            loc = outputs.loc\n",
    "\n",
    "            # Calculate parameters from model outputs\n",
    "            params = (scale * outputs.params[0] + loc, scale * outputs.params[1])\n",
    "            epoch_params.append(params)\n",
    "\n",
    "            # Backward pass and optimization step\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        params_lst.append(epoch_params)  # Store parameters for the epoch\n",
    "        epoch_loss = total_loss / len(train_loader)  # Calculate average loss for the epoch\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    return params_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_config(prediction_length=PRED_LENGTH, context_length=CONTEXT_LENGTH, num_time_features=NUM_TIME_FEAT, \n",
    "                        encoder_layers=ENC_LAYERS, decoder_layers=DEC_LAYERS, d_model=D_MODEL, n_heads=N_HEADS, \n",
    "                        input_size=INPUT_FEAT, lags_sequence=LAG_SEQ, num_parallel_samples=NUM_SAMPLES, distribution_output=DISTR):\n",
    "    \"\"\"\n",
    "    Creates a configuration for a Time Series Transformer model.\n",
    "\n",
    "    Parameters:\n",
    "    prediction_length (int): The number of future time steps the model will predict\n",
    "    context_length (int): The number of past time steps used as context\n",
    "    num_time_features (int): The number of time features used as input\n",
    "    encoder_layers (int): The number of transformer layers in the encoder\n",
    "    decoder_layers (int): The number of transformer layers in the decoder\n",
    "    d_model (int): The dimensionality of the transformer model\n",
    "    n_heads (int): The number of attention heads in the multi-head attention mechanism\n",
    "    input_size (int): The size of the input to the model (number of features per time step)\n",
    "    lags_sequence (list): A list of lags to be used in the model\n",
    "    distribution_output (str): The type of distribution for the output\n",
    "\n",
    "    Returns:\n",
    "    TimeSeriesTransformerConfig: The configuration object for the Time Series Transformer model.\n",
    "    \"\"\"\n",
    "    config = TimeSeriesTransformerConfig(\n",
    "        prediction_length=prediction_length,\n",
    "        context_length=context_length,\n",
    "        num_time_features=num_time_features,\n",
    "        encoder_layers=encoder_layers,\n",
    "        decoder_layers=decoder_layers,\n",
    "        d_model=d_model,\n",
    "        n_heads=n_heads,\n",
    "        input_size=input_size,\n",
    "        lags_sequence=lags_sequence,\n",
    "        distribution_output=distribution_output,\n",
    "    )\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_params(params_lst):\n",
    "    \"\"\"\n",
    "    Processes epoch data and calculates mean and standard deviation of pulse1 and pulse2 for each time point.\n",
    "\n",
    "    This function iterates over each epoch and batch, computes the means and standard deviations of `pulse1` and \n",
    "    `pulse2` for each time point, and stores them in a dictionary. Finally, it averages the values over the number \n",
    "    of batches.\n",
    "\n",
    "    Args:\n",
    "        params_lst (list): A nested list where each element corresponds to an epoch, containing a batch. Each batch\n",
    "                           contains two arrays, one for means and one for standard deviations, with pulse1 and pulse2\n",
    "                           data across time points.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A numpy array of dictionaries, where each dictionary contains the average `mu_pulse1`, \n",
    "                    `sigma_pulse1`, `mu_pulse2`, and `sigma_pulse2` for each time point.\n",
    "    \"\"\"\n",
    "    # Initialize the result dictionary for storing pulse1 and pulse2 data for each time point.\n",
    "    num_epochs = len(params_lst)\n",
    "    num_time_points = len(params_lst[0][0][0][0])\n",
    "    epoch_dict = np.array([\n",
    "        [{\"mu_pulse1\": 0, \"sigma_pulse1\": 0, \"mu_pulse2\": 0, \"sigma_pulse2\": 0}\n",
    "         for _ in range(num_time_points)]\n",
    "        for _ in range(num_epochs)\n",
    "    ])\n",
    "    num_batches = len(params_lst[0])\n",
    "\n",
    "    # Iterate through each epoch and batch to calculate the means and standard deviations\n",
    "    for i, epoch in enumerate(params_lst):\n",
    "        for batch in epoch:\n",
    "            mean, std_dev = batch[0], batch[1]\n",
    "\n",
    "            # Update the epoch dictionary for each time point\n",
    "            for time_point in range(mean.shape[1]):\n",
    "                mu_pulse1 = mean[:, time_point, 0].detach().numpy()\n",
    "                sigma_pulse1 = std_dev[:, time_point, 0].detach().numpy()\n",
    "                mu_pulse2 = mean[:, time_point, 1].detach().numpy()\n",
    "                sigma_pulse2 = std_dev[:, time_point, 1].detach().numpy()\n",
    "\n",
    "                # Accumulate values\n",
    "                epoch_dict[i, time_point][\"mu_pulse1\"] += np.mean(mu_pulse1)\n",
    "                epoch_dict[i, time_point][\"sigma_pulse1\"] += np.mean(sigma_pulse1)\n",
    "                epoch_dict[i, time_point][\"mu_pulse2\"] += np.mean(mu_pulse2)\n",
    "                epoch_dict[i, time_point][\"sigma_pulse2\"] += np.mean(sigma_pulse2)\n",
    "\n",
    "        # Average the accumulated values over the number of batches\n",
    "        for time_point in epoch_dict[i]:\n",
    "            time_point[\"mu_pulse1\"] /= num_batches\n",
    "            time_point[\"sigma_pulse1\"] /= num_batches\n",
    "            time_point[\"mu_pulse2\"] /= num_batches\n",
    "            time_point[\"sigma_pulse2\"] /= num_batches\n",
    "\n",
    "    return pd.DataFrame(epoch_dict.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pulse_mus(epoch_df, ep=NUM_EPOCHS-1, save_path=\"mu_over_timepoints.png\", plot_sigma=True):\n",
    "    \"\"\"\n",
    "    Plots the mean and standard deviation of pulse1 and pulse2 across samples for a specified epoch,\n",
    "    and saves the plot as an image file.\n",
    "\n",
    "    Args:\n",
    "        epoch_df (pd.DataFrame): A DataFrame containing the pulse data across epochs and samples.\n",
    "        ep (int): The epoch to plot (default is the last epoch).\n",
    "        save_path (str): The path where the plot will be saved (default is 'pulse_plot.png').\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    samples = epoch_df.shape[1]\n",
    "    t = np.arange(samples)  \n",
    "    \n",
    "    # Extract mean and sigma values for pulse1 and pulse2 across samples\n",
    "    pulse1_mus = epoch_df.loc[ep, :].apply(lambda x: x[\"mu_pulse1\"]).values\n",
    "    pulse1_sigmas = epoch_df.loc[ep, :].apply(lambda x: x[\"sigma_pulse1\"]).values\n",
    "    pulse2_mus = epoch_df.loc[ep, :].apply(lambda x: x[\"mu_pulse2\"]).values\n",
    "    pulse2_sigmas = epoch_df.loc[ep, :].apply(lambda x: x[\"sigma_pulse2\"]).values\n",
    "\n",
    "    # Plot pulse1 data\n",
    "    plt.figure()\n",
    "    plt.plot(t, pulse1_mus, label=\"Pulse1 Mean\")\n",
    "    if plot_sigma:\n",
    "        plt.fill_between(t, \n",
    "                        pulse1_mus - pulse1_sigmas, \n",
    "                        pulse1_mus + pulse1_sigmas, \n",
    "                        alpha=0.3, label=\"Pulse1 Sigma\")\n",
    "    \n",
    "    # Plot pulse2 data\n",
    "    plt.plot(t, pulse2_mus, label=\"Pulse2 Mean\")\n",
    "    if plot_sigma:\n",
    "        plt.fill_between(t, \n",
    "                        pulse2_mus - pulse2_sigmas, \n",
    "                        pulse2_mus + pulse2_sigmas, \n",
    "                        alpha=0.3, label=\"Pulse2 Sigma\")\n",
    "\n",
    "    # Add labels, legend, and title\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Mean Â± Sigma')\n",
    "    plt.title(f'Pulse1 and Pulse2 Means and Sigmas for Epoch {ep}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "    # Clear the figure to avoid memory issues\n",
    "    plt.close()\n",
    "\n",
    "    return pulse1_mus, pulse2_mus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, test_loader, prediction_length=70):\n",
    "    \"\"\"\n",
    "    Generates predictions for each batch in the test loader using a trained time series model.\n",
    "\n",
    "    Parameters:\n",
    "    model (torch.nn.Module): The trained model used to generate predictions.\n",
    "    test_loader (DataLoader): DataLoader providing batches of test data.\n",
    "    prediction_length (int): The number of future time steps to predict. Default is 70.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of generated predictions for each batch in the test data.\n",
    "    \n",
    "    \"\"\"\n",
    "    generated_predictions = []\n",
    "\n",
    "    # Iterate over each batch in the test loader\n",
    "    for batch in test_loader:\n",
    "        # Split the batch into past and future values\n",
    "        past_values, future_values = split_past_future(batch[0], num_future_points=prediction_length)\n",
    "\n",
    "        # Prepare batch input data\n",
    "        batch = {\n",
    "            \"past_values\": past_values,  # (batch_size, input_length, input_size)\n",
    "            \"future_values\": future_values,  # (batch_size, prediction_length, input_size)\n",
    "            \"past_time_features\": torch.arange(past_values.size(1)).unsqueeze(0).unsqueeze(2).float().repeat(past_values.size(0), 1, 1),  # (batch_size, seq_length, 1)\n",
    "            \"past_observed_mask\": torch.ones_like(past_values),  # (batch_size, seq_length, input_size)\n",
    "            \"future_time_features\": torch.arange(past_values.size(1), past_values.size(1) + prediction_length).unsqueeze(0).unsqueeze(2).float().repeat(future_values.size(0), 1, 1),  # (batch_size, prediction_length, 1)\n",
    "        }\n",
    "\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Generate predictions without gradient calculation\n",
    "        with torch.no_grad():\n",
    "            predictions = model.generate(\n",
    "                past_values=batch[\"past_values\"],\n",
    "                past_time_features=batch[\"past_time_features\"],\n",
    "                future_time_features=batch[\"future_time_features\"],\n",
    "                past_observed_mask=batch[\"past_observed_mask\"],\n",
    "            )\n",
    "\n",
    "        # Collect the generated predictions\n",
    "        generated_batch_predictions = predictions.sequences\n",
    "\n",
    "        # Optional: only append if batch size matches 16\n",
    "        if generated_batch_predictions.shape[0] == 16:\n",
    "            generated_predictions.append(generated_batch_predictions)\n",
    "\n",
    "    return np.array(generated_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_preds_vs_avg(data_avg, generated_predictions, pl=PRED_LENGTH, plt_grand_mean=True, plt_pulse_mus=True, save_path='preds_vs_avg.png'):\n",
    "    \"\"\"\n",
    "    Plots the comparison between predicted and actual values for a specified observation and sample,\n",
    "    alongside statistical averages (mean over epochs and overall grand mean).\n",
    "\n",
    "    Args:\n",
    "        data_avg (np.ndarray): The average data values for past and future sine/cosine values.\n",
    "        generated_predictions (np.ndarray): Generated prediction values of shape (num_batches, batch_size, samples, seq_length, input_size).\n",
    "        pl (int): The prediction length (number of future time steps to plot, default is 70).\n",
    "        plt_grand_mean (bool): Plot mean over predictions and samples (default is True).\n",
    "        plt_pulse_mus (bool): Plot model parameter mu of a specified epoch (default is True).\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Reshape and calculate the means over different dimensions\n",
    "    num_batches, batch_size, samples, seq_length, input_size = generated_predictions.shape\n",
    "    generated_predictions = generated_predictions.reshape(num_batches * batch_size, samples, seq_length, input_size)\n",
    "\n",
    "    # get the mean over all predictions \n",
    "    mean_over_preds = np.mean(generated_predictions, axis=0)\n",
    "    \n",
    "    # get the mean over all samples \n",
    "    mean_over_sampels = np.mean(generated_predictions, axis=1)\n",
    "\n",
    "    # get the mean over all predictions and all samples \n",
    "    grand_mean = np.mean(mean_over_preds, axis=0)\n",
    "\n",
    "    # Plot settings\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot the average past values\n",
    "    plt.plot(np.arange(len(data_avg[:-pl])), data_avg[:-pl, 0], label=\"Average of the past values\")\n",
    "    plt.plot(np.arange(len(data_avg[:-pl])), data_avg[:-pl, 1], label=\"Average of the past values\")\n",
    "\n",
    "    # Plot the average future values\n",
    "    plt.plot(np.arange(len(data_avg[:-pl]), len(data_avg[:-pl]) + pl), data_avg[-pl:, 0], label=\"Average of future values\", linestyle='dashed')\n",
    "    plt.plot(np.arange(len(data_avg[:-pl]), len(data_avg[:-pl]) + pl), data_avg[-pl:, 1], label=\"Average of future values\", linestyle='dashed')\n",
    "\n",
    "    if plt_grand_mean:\n",
    "        # Plot the mean over all predictions and all samples \n",
    "        plt.plot(np.arange(len(data_avg[:-pl]), len(data_avg[:-pl]) + pl), grand_mean[:, 0], label=\"Mean over predictions and samples\", linestyle=\"dotted\")\n",
    "        plt.plot(np.arange(len(data_avg[:-pl]), len(data_avg[:-pl]) + pl), grand_mean[:, 1], label=\"Mean over predictions and samples\", linestyle=\"dotted\")\n",
    "    \n",
    "    if plt_pulse_mus:\n",
    "        # Plot the model parameter mu of a specified epoch \n",
    "        plt.plot(np.arange(len(data_avg[:-pl]), len(data_avg[:-pl]) + pl), pulse1_mus, label=\"Model parameter mu\", linestyle=\"dotted\")\n",
    "        plt.plot(np.arange(len(data_avg[:-pl]), len(data_avg[:-pl]) + pl), pulse2_mus, label=\"Model parameter mu\", linestyle=\"dotted\")\n",
    "\n",
    "    # Add labels, legend, and title\n",
    "    plt.xlabel('Timepoints')\n",
    "    plt.ylabel('Values')\n",
    "    plt.title(f'Average data against prediction')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "    # Clear the figure to avoid memory issues\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate pulse data \n",
    "data = generate_pulse_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and test set \n",
    "X_train, y = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and test loader, batch the data \n",
    "train_loader, test_loader = create_data_loaders(X_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the configuration of the model \n",
    "config = create_model_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model \n",
    "model = TimeSeriesTransformerForPrediction(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model \n",
    "model = TimeSeriesTransformerForPrediction(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 2.9656\n",
      "Epoch 2/20, Loss: 2.8992\n",
      "Epoch 3/20, Loss: 2.8833\n",
      "Epoch 4/20, Loss: 2.8773\n",
      "Epoch 5/20, Loss: 2.8753\n",
      "Epoch 6/20, Loss: 2.8726\n",
      "Epoch 7/20, Loss: 2.8710\n",
      "Epoch 8/20, Loss: 2.8687\n",
      "Epoch 9/20, Loss: 2.8712\n",
      "Epoch 10/20, Loss: 2.8676\n",
      "Epoch 11/20, Loss: 2.8687\n",
      "Epoch 12/20, Loss: 2.8662\n",
      "Epoch 13/20, Loss: 2.8662\n",
      "Epoch 14/20, Loss: 2.8677\n",
      "Epoch 15/20, Loss: 2.8655\n",
      "Epoch 16/20, Loss: 2.8645\n",
      "Epoch 17/20, Loss: 2.8645\n",
      "Epoch 18/20, Loss: 2.8641\n",
      "Epoch 19/20, Loss: 2.8652\n",
      "Epoch 20/20, Loss: 2.8645\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "params = train_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model \n",
    "model.save_pretrained(\"./saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrange the parameters in a pd\n",
    "params_df = arrange_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mu over time points of specified epoch \n",
    "pulse1_mus, pulse2_mus = plot_pulse_mus(params_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# generate predictions \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 35\u001b[0m, in \u001b[0;36mgenerate_predictions\u001b[0;34m(model, test_loader, prediction_length)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Generate predictions without gradient calculation\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 35\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpast_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpast_time_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuture_time_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfuture_time_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_observed_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpast_observed_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Collect the generated predictions\u001b[39;00m\n\u001b[1;32m     43\u001b[0m generated_batch_predictions \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39msequences\n",
      "File \u001b[0;32m/data/u_kamm_software/EEG data analysis/eeg_llm/.venv/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/u_kamm_software/EEG data analysis/eeg_llm/.venv/lib/python3.9/site-packages/transformers/models/time_series_transformer/modeling_time_series_transformer.py:1763\u001b[0m, in \u001b[0;36mTimeSeriesTransformerForPrediction.generate\u001b[0;34m(self, past_values, past_time_features, future_time_features, past_observed_mask, static_categorical_features, static_real_features, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m   1759\u001b[0m reshaped_lagged_sequence \u001b[38;5;241m=\u001b[39m lagged_sequence\u001b[38;5;241m.\u001b[39mreshape(lags_shape[\u001b[38;5;241m0\u001b[39m], lags_shape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1761\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((reshaped_lagged_sequence, repeated_features[:, : k \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1763\u001b[0m dec_output \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeated_enc_last_hidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m dec_last_hidden \u001b[38;5;241m=\u001b[39m dec_output\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m   1766\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameter_projection(dec_last_hidden[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[0;32m/data/u_kamm_software/EEG data analysis/eeg_llm/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/u_kamm_software/EEG data analysis/eeg_llm/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/u_kamm_software/EEG data analysis/eeg_llm/.venv/lib/python3.9/site-packages/transformers/models/time_series_transformer/modeling_time_series_transformer.py:1132\u001b[0m, in \u001b[0;36mTimeSeriesTransformerDecoder.forward\u001b[0;34m(self, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1120\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1121\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         use_cache,\n\u001b[1;32m   1130\u001b[0m     )\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1145\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/data/u_kamm_software/EEG data analysis/eeg_llm/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/u_kamm_software/EEG data analysis/eeg_llm/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/u_kamm_software/EEG data analysis/eeg_llm/.venv/lib/python3.9/site-packages/transformers/models/time_series_transformer/modeling_time_series_transformer.py:596\u001b[0m, in \u001b[0;36mTimeSeriesTransformerDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;66;03m# cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\u001b[39;00m\n\u001b[1;32m    595\u001b[0m cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 596\u001b[0m hidden_states, cross_attn_weights, cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    605\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/data/u_kamm_software/EEG data analysis/eeg_llm/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/u_kamm_software/EEG data analysis/eeg_llm/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/u_kamm_software/EEG data analysis/eeg_llm/.venv/lib/python3.9/site-packages/transformers/models/time_series_transformer/modeling_time_series_transformer.py:346\u001b[0m, in \u001b[0;36mTimeSeriesTransformerAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_cross_attention:\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;66;03m# cross_attentions\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(key_value_states), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bsz)\n\u001b[0;32m--> 346\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;66;03m# reuse k, v, self_attention\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bsz)\n",
      "File \u001b[0;32m/data/u_kamm_software/EEG data analysis/eeg_llm/.venv/lib/python3.9/site-packages/transformers/models/time_series_transformer/modeling_time_series_transformer.py:310\u001b[0m, in \u001b[0;36mTimeSeriesTransformerAttention._shape\u001b[0;34m(self, tensor, seq_len, bsz)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_shape\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: torch\u001b[38;5;241m.\u001b[39mTensor, seq_len: \u001b[38;5;28mint\u001b[39m, bsz: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# generate predictions \n",
    "predictions = generate_predictions(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
