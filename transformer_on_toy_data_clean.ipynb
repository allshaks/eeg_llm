{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/u_kamm_software/EEG data analysis/eeg_llm/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerForPrediction\n",
    "import numpy as np \n",
    "from torch.optim import Adam \n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define global variables \n",
    "NUM_OBSV = 1000                     # number of generated observations  \n",
    "SEQ_LEN = 100                       # length of the sequence \n",
    "NOISE = 1                           # level of noise \n",
    "MEAN1 = 1                           # mean of the first pulse \n",
    "STD1 = 0.3                          # standard deviation of the first pulse \n",
    "MEAN2 = 2                           # mean of the second pulse \n",
    "STD2 = 0.3                          # standard deviation of the second pulse \n",
    "TEST_SIZE = 0.2                     # size of the test set \n",
    "BATCH_SIZE = 16                     # batch size \n",
    "RANDOM_STATE = 42                   # random initialization for reproducability \n",
    "PRED_LENGTH = 10                    # length of the prediction \n",
    "CONTEXT_LENGTH = 89                 # length of the context window \n",
    "NUM_TIME_FEAT = 1                   # number of time features \n",
    "ENC_LAYERS = 2                      # number of encoding layers \n",
    "DEC_LAYERS = 2                      # number of decoding layers \n",
    "INPUT_FEAT = 2                      # number of input features \n",
    "LAG_SEQ = [1]                       # lag sequence \n",
    "D_MODEL = 64                        # dimension of the model \n",
    "N_HEADS = 8                         # number of attention heads\n",
    "DISTR = 'normal'                    # output distribution  \n",
    "NUM_SAMPLES = 10                    # number of parallel generated samples \n",
    "NUM_EPOCHS = 20                     # number of epochs \n",
    "LEARNING_RATE = 1e-3                # learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(t, mean, std):\n",
    "    \"\"\"\n",
    "    Computes the Gaussian (normal distribution) value for each point in time.\n",
    "\n",
    "    Parameters:\n",
    "    t (array-like): The input time points at which to evaluate the Gaussian function.\n",
    "    mean (float): The mean (center) of the Gaussian distribution.\n",
    "    std (float): The standard deviation (spread or width) of the Gaussian distribution.\n",
    "\n",
    "    Returns:\n",
    "    ndarray: A 1D array of shape (t,)\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1/np.sqrt(2*np.pi*std**2)*np.exp(-(t-mean)**2/(2*std**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pulse_data(num_samples=NUM_OBSV, seq_len=SEQ_LEN, noise=NOISE):\n",
    "    \"\"\"\n",
    "    Generates synthetic pulse data with time-shifted Gaussian pulses and random noise.\n",
    "\n",
    "    Parameters:\n",
    "    num_samples (int): The number of pulse samples to generate. Each sample contains two pulses. \n",
    "    seq_length (int): The number of time points in each pulse sequence.\n",
    "    noise (float): The standard deviation of the noise added to the pulse amplitudes. \n",
    "\n",
    "    Returns:\n",
    "    ndarray: A 3D array of shape (num_samples, seq_length, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        phase_shift = np.random.randn(1)\n",
    "        t = np.linspace(0, 5, seq_len) + phase_shift\n",
    "\n",
    "        pulse1 = gaussian(t, MEAN1, STD1) + noise * np.random.randn(seq_len)\n",
    "        pulse2 = gaussian(t, MEAN2, STD2) + noise * np.random.randn(seq_len)\n",
    "\n",
    "        sample = np.stack([pulse1, pulse2], axis=1)\n",
    "        data.append(sample)\n",
    "    data = np.array(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, test_size=TEST_SIZE, random_state=RANDOM_STATE):\n",
    "    \"\"\"\n",
    "    Splits the input data into training and testing set.\n",
    "\n",
    "    Parameters:\n",
    "    data (array-like): The input data to be split.\n",
    "    test_size (float): The proportion of the data to include in the test split.\n",
    "    random_state (int, optional): Controls the shuffling applied to the data before the split. \n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - X_train (array-like): A 3D array of shape (num_samples - test_size * num_samples, seq_length, 2).\n",
    "        - y (array-like): A 3D array of shape (test_size * num_samples, seq_length, 2).\n",
    "    \"\"\"\n",
    "\n",
    "    X_train, y = train_test_split(data, test_size=test_size, random_state=random_state)\n",
    "    return X_train, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(X_train, X_test, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    \"\"\"\n",
    "    Converts training and testing data into PyTorch DataLoader objects.\n",
    "\n",
    "    Parameters:\n",
    "    X_train (array-like): The training data to be converted into a DataLoader.\n",
    "    X_test (array-like): The test data to be converted into a DataLoader.\n",
    "    batch_size (int): The number of samples per batch to load.\n",
    "    shuffle (bool): Whether to shuffle the training data at every epoch. Default is True.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - train_loader (DataLoader): DataLoader for the training data.\n",
    "        - test_loader (DataLoader): DataLoader for the test data (shuffling disabled).\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    train_dataset = TensorDataset(X_train)\n",
    "    test_dataset = TensorDataset(X_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_past_future(batch, num_future_points=PRED_LENGTH):\n",
    "    \"\"\"\n",
    "    Splits a batch of time series data into past and future segments.\n",
    "\n",
    "    Parameters:\n",
    "    batch (ndarray): A batch of time series data of shape (batch_size, sequence_length, num_features).\n",
    "    num_future_points (int): The number of future time points to separate from the past.\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - past_values (ndarray): The past segment of the data, with shape (batch_size, sequence_length - num_future_points, num_features).\n",
    "        - future_values (ndarray): The future segment of the data, with shape (batch_size, num_future_points, num_features).\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    past_values = batch[:, :-num_future_points, :]\n",
    "    future_values = batch[:, -num_future_points:, :]\n",
    "    return past_values, future_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, num_future_points=PRED_LENGTH, num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE):\n",
    "    \"\"\"\n",
    "    Trains the model using the provided training data loader.\n",
    "\n",
    "    Parameters:\n",
    "    model (torch.nn.Module): The model to be trained.\n",
    "    train_loader (DataLoader): The DataLoader providing batches of training data.\n",
    "    num_future_points (int): The number of future time steps the model will predict.\n",
    "    num_epochs (int): The number of training epochs.\n",
    "    learning_rate (float): The learning rate for the optimizer.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of parameters (scale and location) for each epoch.\n",
    "    \"\"\"\n",
    "    # Initialize optimizer\n",
    "    optim = Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Store parameters across epochs\n",
    "    params_lst = []\n",
    "\n",
    "    # Iterate over all epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        total_loss = 0.0  # Initialize total loss for the epoch\n",
    "        epoch_params = []  # Store parameters for the current epoch\n",
    "\n",
    "        # Iterate over each batch in the training data\n",
    "        for batch in train_loader:\n",
    "            past_values, future_values = split_past_future(batch[0], num_future_points=num_future_points)\n",
    "\n",
    "            # Prepare batch data for model input\n",
    "            batch = {\n",
    "                \"past_values\": past_values,  # (batch_size, input_length, input_size)\n",
    "                \"future_values\": future_values,  # (batch_size, prediction_length, input_size)\n",
    "                \"past_time_features\": torch.arange(past_values.size(1)).unsqueeze(0).unsqueeze(2).float().repeat(past_values.size(0), 1, 1),  # (batch_size, seq_length, 1)\n",
    "                \"past_observed_mask\": torch.ones_like(past_values),  # (batch_size, seq_length, input_size)\n",
    "                \"future_observed_mask\": torch.ones_like(future_values),  # (batch_size, prediction_length, input_size)\n",
    "                \"future_time_features\": torch.arange(past_values.size(1), past_values.size(1) + num_future_points).unsqueeze(0).unsqueeze(2).float().repeat(future_values.size(0), 1, 1),  # (batch_size, prediction_length, 1)\n",
    "                \"return_dict\": True\n",
    "            }\n",
    "\n",
    "            # Forward pass through the model\n",
    "            outputs = model(\n",
    "                past_values=batch[\"past_values\"],\n",
    "                past_time_features=batch[\"past_time_features\"],\n",
    "                past_observed_mask=batch[\"past_observed_mask\"],\n",
    "                future_observed_mask=batch[\"future_observed_mask\"],\n",
    "                future_values=batch[\"future_values\"],\n",
    "                future_time_features=batch[\"future_time_features\"],\n",
    "                return_dict=batch[\"return_dict\"]\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            scale = outputs.scale\n",
    "            loc = outputs.loc\n",
    "\n",
    "            # Calculate parameters from model outputs\n",
    "            params = (scale * outputs.params[0] + loc, scale * outputs.params[1])\n",
    "            epoch_params.append(params)\n",
    "\n",
    "            # Backward pass and optimization step\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        params_lst.append(epoch_params)  # Store parameters for the epoch\n",
    "        epoch_loss = total_loss / len(train_loader)  # Calculate average loss for the epoch\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    return params_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_config(prediction_length=PRED_LENGTH, context_length=CONTEXT_LENGTH, num_time_features=NUM_TIME_FEAT, \n",
    "                        encoder_layers=ENC_LAYERS, decoder_layers=DEC_LAYERS, d_model=D_MODEL, n_heads=N_HEADS, \n",
    "                        input_size=INPUT_FEAT, lags_sequence=LAG_SEQ, num_parallel_samples=NUM_SAMPLES, distribution_output=DISTR):\n",
    "    \"\"\"\n",
    "    Creates a configuration for a Time Series Transformer model.\n",
    "\n",
    "    Parameters:\n",
    "    prediction_length (int): The number of future time steps the model will predict\n",
    "    context_length (int): The number of past time steps used as context\n",
    "    num_time_features (int): The number of time features used as input\n",
    "    encoder_layers (int): The number of transformer layers in the encoder\n",
    "    decoder_layers (int): The number of transformer layers in the decoder\n",
    "    d_model (int): The dimensionality of the transformer model\n",
    "    n_heads (int): The number of attention heads in the multi-head attention mechanism\n",
    "    input_size (int): The size of the input to the model (number of features per time step)\n",
    "    lags_sequence (list): A list of lags to be used in the model\n",
    "    num_parallel_samples (int): number of samples to generate for found parameters \n",
    "    distribution_output (str): The type of distribution for the output\n",
    "\n",
    "    Returns:\n",
    "    TimeSeriesTransformerConfig: The configuration object for the Time Series Transformer model.\n",
    "    \"\"\"\n",
    "    config = TimeSeriesTransformerConfig(\n",
    "        prediction_length=prediction_length,\n",
    "        context_length=context_length,\n",
    "        num_time_features=num_time_features,\n",
    "        encoder_layers=encoder_layers,\n",
    "        decoder_layers=decoder_layers,\n",
    "        d_model=d_model,\n",
    "        n_heads=n_heads,\n",
    "        input_size=input_size,\n",
    "        lags_sequence=lags_sequence,\n",
    "        num_parallel_samples=num_parallel_samples,\n",
    "        distribution_output=distribution_output,\n",
    "    )\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_params(params_lst):\n",
    "    \"\"\"\n",
    "    Processes epoch data and calculates mean and standard deviation of pulse1 and pulse2 for each time point.\n",
    "\n",
    "    This function iterates over each epoch and batch, computes the means and standard deviations of `pulse1` and \n",
    "    `pulse2` for each time point, and stores them in a dictionary. Finally, it averages the values over the number \n",
    "    of batches.\n",
    "\n",
    "    Args:\n",
    "        params_lst (list): A nested list where each element corresponds to an epoch, containing a batch. Each batch\n",
    "                           contains two arrays, one for means and one for standard deviations, with pulse1 and pulse2\n",
    "                           data across time points.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A numpy array of dictionaries, where each dictionary contains the average `mu_pulse1`, \n",
    "                    `sigma_pulse1`, `mu_pulse2`, and `sigma_pulse2` for each time point.\n",
    "    \"\"\"\n",
    "    # Initialize the result dictionary for storing pulse1 and pulse2 data for each time point.\n",
    "    num_epochs = len(params_lst)\n",
    "    num_time_points = len(params_lst[0][0][0][0])\n",
    "    epoch_dict = np.array([\n",
    "        [{\"mu_pulse1\": 0, \"sigma_pulse1\": 0, \"mu_pulse2\": 0, \"sigma_pulse2\": 0}\n",
    "         for _ in range(num_time_points)]\n",
    "        for _ in range(num_epochs)\n",
    "    ])\n",
    "    num_batches = len(params_lst[0])\n",
    "\n",
    "    # Iterate through each epoch and batch to calculate the means and standard deviations\n",
    "    for i, epoch in enumerate(params_lst):\n",
    "        for batch in epoch:\n",
    "            mean, std_dev = batch[0], batch[1]\n",
    "\n",
    "            # Update the epoch dictionary for each time point\n",
    "            for time_point in range(mean.shape[1]):\n",
    "                mu_pulse1 = mean[:, time_point, 0].detach().numpy()\n",
    "                sigma_pulse1 = std_dev[:, time_point, 0].detach().numpy()\n",
    "                mu_pulse2 = mean[:, time_point, 1].detach().numpy()\n",
    "                sigma_pulse2 = std_dev[:, time_point, 1].detach().numpy()\n",
    "\n",
    "                # Accumulate values\n",
    "                epoch_dict[i, time_point][\"mu_pulse1\"] += np.mean(mu_pulse1)\n",
    "                epoch_dict[i, time_point][\"sigma_pulse1\"] += np.mean(sigma_pulse1)\n",
    "                epoch_dict[i, time_point][\"mu_pulse2\"] += np.mean(mu_pulse2)\n",
    "                epoch_dict[i, time_point][\"sigma_pulse2\"] += np.mean(sigma_pulse2)\n",
    "\n",
    "        # Average the accumulated values over the number of batches\n",
    "        for time_point in epoch_dict[i]:\n",
    "            time_point[\"mu_pulse1\"] /= num_batches\n",
    "            time_point[\"sigma_pulse1\"] /= num_batches\n",
    "            time_point[\"mu_pulse2\"] /= num_batches\n",
    "            time_point[\"sigma_pulse2\"] /= num_batches\n",
    "\n",
    "    return pd.DataFrame(epoch_dict.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pulse_mus(epoch_df, ep=NUM_EPOCHS-1, save_path=\"mu_over_timepoints.png\", plot_sigma=True):\n",
    "    \"\"\"\n",
    "    Plots the mean and standard deviation of pulse1 and pulse2 across samples for a specified epoch,\n",
    "    and saves the plot as an image file.\n",
    "\n",
    "    Args:\n",
    "        epoch_df (pd.DataFrame): A DataFrame containing the pulse data across epochs and samples.\n",
    "        ep (int): The epoch to plot (default is the last epoch).\n",
    "        save_path (str): The path where the plot will be saved (default is 'pulse_plot.png').\n",
    "        plt_sigma (bool): Plot the standard deviation (default is True). \n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    samples = epoch_df.shape[1]\n",
    "    t = np.arange(samples)  \n",
    "    \n",
    "    # Extract mean and sigma values for pulse1 and pulse2 across samples\n",
    "    pulse1_mus = epoch_df.loc[ep, :].apply(lambda x: x[\"mu_pulse1\"]).values\n",
    "    pulse1_sigmas = epoch_df.loc[ep, :].apply(lambda x: x[\"sigma_pulse1\"]).values\n",
    "    pulse2_mus = epoch_df.loc[ep, :].apply(lambda x: x[\"mu_pulse2\"]).values\n",
    "    pulse2_sigmas = epoch_df.loc[ep, :].apply(lambda x: x[\"sigma_pulse2\"]).values\n",
    "\n",
    "    # Plot pulse1 data\n",
    "    plt.figure()\n",
    "    plt.plot(t, pulse1_mus, label=\"Pulse1 Mean\")\n",
    "    if plot_sigma:\n",
    "        plt.fill_between(t, \n",
    "                        pulse1_mus - pulse1_sigmas, \n",
    "                        pulse1_mus + pulse1_sigmas, \n",
    "                        alpha=0.3, label=\"Pulse1 Sigma\")\n",
    "    \n",
    "    # Plot pulse2 data\n",
    "    plt.plot(t, pulse2_mus, label=\"Pulse2 Mean\")\n",
    "    if plot_sigma:\n",
    "        plt.fill_between(t, \n",
    "                        pulse2_mus - pulse2_sigmas, \n",
    "                        pulse2_mus + pulse2_sigmas, \n",
    "                        alpha=0.3, label=\"Pulse2 Sigma\")\n",
    "\n",
    "    # Add labels, legend, and title\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Mean Â± Sigma')\n",
    "    plt.title(f'Pulse1 and Pulse2 Means and Sigmas for Epoch {ep}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "    # Clear the figure to avoid memory issues\n",
    "    plt.close()\n",
    "\n",
    "    return pulse1_mus, pulse2_mus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, test_loader, prediction_length=PRED_LENGTH):\n",
    "    \"\"\"\n",
    "    Generates predictions for each batch in the test loader using a trained time series model.\n",
    "\n",
    "    Parameters:\n",
    "    model (torch.nn.Module): The trained model used to generate predictions.\n",
    "    test_loader (DataLoader): DataLoader providing batches of test data.\n",
    "    prediction_length (int): The number of future time steps to predict.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of generated predictions for each batch in the test data.\n",
    "    \n",
    "    \"\"\"\n",
    "    generated_predictions = []\n",
    "\n",
    "    # Iterate over each batch in the test loader\n",
    "    for batch in test_loader:\n",
    "        # Split the batch into past and future values\n",
    "        past_values, future_values = split_past_future(batch[0], num_future_points=prediction_length)\n",
    "\n",
    "        # Prepare batch input data\n",
    "        batch = {\n",
    "            \"past_values\": past_values,  # (batch_size, input_length, input_size)\n",
    "            \"future_values\": future_values,  # (batch_size, prediction_length, input_size)\n",
    "            \"past_time_features\": torch.arange(past_values.size(1)).unsqueeze(0).unsqueeze(2).float().repeat(past_values.size(0), 1, 1),  # (batch_size, seq_length, 1)\n",
    "            \"past_observed_mask\": torch.ones_like(past_values),  # (batch_size, seq_length, input_size)\n",
    "            \"future_time_features\": torch.arange(past_values.size(1), past_values.size(1) + prediction_length).unsqueeze(0).unsqueeze(2).float().repeat(future_values.size(0), 1, 1),  # (batch_size, prediction_length, 1)\n",
    "        }\n",
    "\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Generate predictions without gradient calculation\n",
    "        with torch.no_grad():\n",
    "            predictions = model.generate(\n",
    "                past_values=batch[\"past_values\"],\n",
    "                past_time_features=batch[\"past_time_features\"],\n",
    "                future_time_features=batch[\"future_time_features\"],\n",
    "                past_observed_mask=batch[\"past_observed_mask\"],\n",
    "            )\n",
    "\n",
    "        # Collect the generated predictions\n",
    "        generated_batch_predictions = predictions.sequences\n",
    "\n",
    "        # Optional: only append if batch size matches 16\n",
    "        if generated_batch_predictions.shape[0] == 16:\n",
    "            generated_predictions.append(generated_batch_predictions)\n",
    "\n",
    "    return np.array(generated_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_preds_vs_avg(data_avg, preds, pulse1_mus, pulse2_mus, pl=PRED_LENGTH, plt_grand_mean=True, plt_pulse_mus=True, save_path='preds_vs_avg.png'):\n",
    "    \"\"\"\n",
    "    Plots the comparison between average data and the grand mean of the predictions and optionally also the \n",
    "    found parameters. \n",
    "\n",
    "    Args:\n",
    "        data_avg (np.ndarray): The average data values for past and future values.\n",
    "        generated_predictions (np.ndarray): Generated prediction values of shape (num_batches, batch_size, samples, seq_length, input_size).\n",
    "        pl (int): The prediction length.\n",
    "        plt_grand_mean (bool): Plot mean over predictions and samples (default is True).\n",
    "        plt_pulse_mus (bool): Plot model parameter mu of a specified epoch (default is True).\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Reshape and calculate the means over different dimensions\n",
    "    num_batches, batch_size, samples, seq_length, input_size = preds.shape\n",
    "    preds = preds.reshape(num_batches * batch_size, samples, seq_length, input_size)\n",
    "\n",
    "    # get the mean over all predictions \n",
    "    mean_over_preds = np.mean(preds, axis=0)\n",
    "    \n",
    "    # get the mean over all samples \n",
    "    mean_over_sampels = np.mean(preds, axis=1)\n",
    "\n",
    "    # get the mean over all predictions and all samples \n",
    "    grand_mean = np.mean(mean_over_preds, axis=0)\n",
    "\n",
    "    # Plot settings\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot the average values\n",
    "    plt.plot(np.arange(len(data_avg)), data_avg[:, 0], label=\"Average\")\n",
    "    plt.plot(np.arange(len(data_avg)), data_avg[:, 1], label=\"Average\")\n",
    "\n",
    "    if plt_grand_mean:\n",
    "        # Plot the mean over all predictions and all samples \n",
    "        plt.plot(np.arange(len(data_avg[:-pl]), len(data_avg[:-pl]) + pl), grand_mean[:, 0], label=\"Mean over predictions and samples\", linestyle=\"dotted\")\n",
    "        plt.plot(np.arange(len(data_avg[:-pl]), len(data_avg[:-pl]) + pl), grand_mean[:, 1], label=\"Mean over predictions and samples\", linestyle=\"dotted\")\n",
    "    \n",
    "    if plt_pulse_mus:\n",
    "        # Plot the model parameter mu of a specified epoch \n",
    "        plt.plot(np.arange(len(data_avg[:-pl]), len(data_avg[:-pl]) + pl), pulse1_mus, label=\"Model parameter mu\", linestyle=\"dotted\")\n",
    "        plt.plot(np.arange(len(data_avg[:-pl]), len(data_avg[:-pl]) + pl), pulse2_mus, label=\"Model parameter mu\", linestyle=\"dotted\")\n",
    "\n",
    "    # Add labels, legend, and title\n",
    "    plt.xlabel('Timepoints')\n",
    "    plt.ylabel('Values')\n",
    "    plt.title(f'Average data against prediction')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "    # Clear the figure to avoid memory issues\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate pulse data \n",
    "data = generate_pulse_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average data \n",
    "data_avg = np.mean(data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and test set \n",
    "X_train, y = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and test loader, batch the data \n",
    "train_loader, test_loader = create_data_loaders(X_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the configuration of the model \n",
    "config = create_model_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model \n",
    "model = TimeSeriesTransformerForPrediction(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model \n",
    "model = TimeSeriesTransformerForPrediction(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 2.8576\n",
      "Epoch 2/20, Loss: 2.8524\n",
      "Epoch 3/20, Loss: 2.8474\n",
      "Epoch 4/20, Loss: 2.8493\n",
      "Epoch 5/20, Loss: 2.8467\n",
      "Epoch 6/20, Loss: 2.8475\n",
      "Epoch 7/20, Loss: 2.8479\n",
      "Epoch 8/20, Loss: 2.8474\n",
      "Epoch 9/20, Loss: 2.8475\n",
      "Epoch 10/20, Loss: 2.8470\n",
      "Epoch 11/20, Loss: 2.8464\n",
      "Epoch 12/20, Loss: 2.8464\n",
      "Epoch 13/20, Loss: 2.8456\n",
      "Epoch 14/20, Loss: 2.8468\n",
      "Epoch 15/20, Loss: 2.8469\n",
      "Epoch 16/20, Loss: 2.8476\n",
      "Epoch 17/20, Loss: 2.8466\n",
      "Epoch 18/20, Loss: 2.8461\n",
      "Epoch 19/20, Loss: 2.8465\n",
      "Epoch 20/20, Loss: 2.8463\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "params = train_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model \n",
    "model.save_pretrained(\"./saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrange the parameters in a pd\n",
    "params_df = arrange_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mu over time points of specified epoch \n",
    "pulse1_mus, pulse2_mus = plot_pulse_mus(params_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions \n",
    "predictions = generate_predictions(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions against average data \n",
    "plot_preds_vs_avg(data_avg=data_avg, preds=predictions, pulse1_mus=pulse1_mus, pulse2_mus=pulse2_mus) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
